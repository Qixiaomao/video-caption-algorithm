![alt text](image-1.png)

# Video Captioning System with ViT + GPT-2 and Chainlit Front-end

This repository contains the full implementation of my Master's graduation project:  
**An end-to-end Video Captioning System integrating a ViT-based video encoder, GPT-2 text decoder, and a Chainlit-powered front-end for interactive inference.**

The project provides:
- A complete video captioning model  
- Data preprocessing pipeline  
- Training and evaluation scripts  
- A user-friendly Chainlit demo interface  
- Human evaluation results for caption quality  

---

## ğŸš€ Features

- **Vision Transformer (ViT) Encoder**
- **GPT-2 Language Decoder**
- **CLIP-style Multimodal Projection Head**
- **Frame-based Video Preprocessing**
- **Chainlit UI for Real-time Caption Generation**
- **Human Evaluation using Fluency / Relevance / Specificity / Overall Preference**
- **Support for MSVD Dataset**

---
#### ğŸ“ˆ Model Architecture

![alt text](image-2.png)


---

####  ğŸ“‚ Project Structure

```c:
project_root/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â””â”€â”€ processed/
â”‚ â””â”€â”€ msvd/
â”‚ â”œâ”€â”€ train.json
â”‚ â”œâ”€â”€ val.json
â”‚ â”œâ”€â”€ test.json
â”‚ â””â”€â”€ frames/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ dataloaders/
â”‚ â”œâ”€â”€ cli/
â”‚ â”‚ â”œâ”€â”€ train.py
â”‚ â”‚ â””â”€â”€ inference.py
â”‚ â””â”€â”€ utils/
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ msvd_prepare.py
â”‚ â””â”€â”€ generate_human_eval.py
â”‚
â”œâ”€â”€ Ui/
â”‚ â””â”€â”€ app_chainlit.py
â”‚
â”œâ”€â”€ outputs/
â”‚ â””â”€â”€ checkpoints/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```



---

ğŸ¤– Inference (CLI)

Example:

```c:
python src/cli/inference.py \
    --video_path example.mp4 \
    --checkpoint outputs/checkpoints/best.ckpt \
    --num_frames 16

```


Outputs:

```css:
Generated caption: "A woman is preparing food in the kitchen."
```

---

ğŸ“Š Human Evaluation

A human evaluation survey was conducted using 4 criteria:

**Fluency,Relevance,Specificity,Overall Preference**

Example summary results:

| Criterion          | Avg. Score |
| ------------------ | ---------- |
| Fluency            | 3.38       |
| Relevance          | 2.63       |
| Specificity        | 3.25       |
| Overall Preference | 4.00       |

---
ğŸ’¬ Chainlit Demo (Front-end)

To launch the interactive UI:
```c:
chainlit run Ui/app_chainlit.py -w
```


Then open the local URL shown in the terminal.

In the UI, you can:

Select an inference engine

Paste a video frame directory path

Generate captions interactively


---
ğŸ” TODO

- Add support for audio-based captioning

- Extend dataset to MSR-VTT

- Improve Chainlit UI for video uploading

- Add BLEU/ROUGE automatic metrics to the demo

- Deploy model via FastAPI backend

---
### How to start

1. Clone the repository:
```c:
1. git clone

2. cd video-captioning-project

3. chainlit run chainlit_app.py -w

```