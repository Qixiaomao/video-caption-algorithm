## 18/09/25 Experiment log

## ä¸‹è½½æ•°æ®é›†
- å¤„ç†æ•°æ®é›† kaggle msvd 
- æ¸…æ´—å­—å¹•éƒ¨åˆ†æ•°æ®
- æŒ‰video_id æ‰¹é‡ä»ŽYouTubeæ‹‰åŽŸè§†é¢‘å¹¶è£å‰ªç‰‡æ®µ

### ç»ˆç«¯è®°å½•
d_prepare.py --raw_dir .\data\raw\msvd\ --out_dir .\data\processed\msvd --format grouped       
[ERROR] æœªèƒ½ä»Ž data\raw\msvd\annotations.txt è§£æžå‡ºä»»ä½• (video_id, caption)python .\scripts\msvd_prepare.py --raw_dir .\data\raw\msvd\ --out_dir .\data\processed\msvd --format groupedS D:\Mylearn\INTI courses\Graduation thesis\video-captioning-transformer>
[INFO] loaded caption rows: 80827
[INFO] unique videos in annotations: 1970
[INFO] indexed video files: 0
[OK] train: 1576 -> data\processed\msvd\train\annotations.json
[OK] val: 197 -> data\processed\msvd\val\annotations.json
[OK] test: 197 -> data\processed\msvd\test\annotations.json
PS D:\Mylearn\INTI courses\Graduation thesis\video-captioning-transformer> python .\check_videos.py
æ€»å…±æ‰¾åˆ°è§†é¢‘æ–‡ä»¶: 0
å‰20ä¸ªæ–‡ä»¶ç¤ºä¾‹ï¼š


## å¯¹æ¯å¸§è§†é¢‘è¿›è¡Œé¢„å¤„ç†ï¼ŒåŠ è½½å™¨è¯»å–å°æ ·æœ¬è§†é¢‘ï¼Œå¹¶è£å‰ªæˆå›ºå®šå¤§å°çš„ç‰‡æ®µ

PS D:\Mylearn\INTI courses\Graduation thesis\video-captioning-transformer> python .\scripts\msvd_compat_frame_names_plus.py
[train] å…¼å®¹å‘½åå®Œæˆçš„ç›®å½•æ•°: 260
[val] å…¼å®¹å‘½åå®Œæˆçš„ç›®å½•æ•°: 0
[test] å…¼å®¹å‘½åå®Œæˆçš„ç›®å½•æ•°: 0
[DONE] å…¼å®¹å‘½å V2 å®Œæˆ
PS D:\Mylearn\INTI courses\Graduation thesis\video-captioning-transformer> python -m src.test_loader
[INFO] Using HuggingFace BertTokenizerFast.
[INFO] build_dataloader signature: (ann_path: str, tokenizer, batch_size: int = 2, max_len: int = 32, num_frame: int = 8, image_size: int = 224, shuffle: bool = False, num_wokers: int = 0)  
[INFO] DataLoader created. Iterate a few batches...
---- Batch 0 ----
video: <class 'torch.Tensor'> torch.Size([2, 8, 3, 224, 224])
caption_ids: <class 'torch.Tensor'> torch.Size([2, 32])
video_id: <class 'list'> None
---- Batch 1 ----
video: <class 'torch.Tensor'> torch.Size([2, 8, 3, 224, 224])
caption_ids: <class 'torch.Tensor'> torch.Size([2, 32])
video_id: <class 'list'> None
---- Batch 2 ----
video: <class 'torch.Tensor'> torch.Size([2, 8, 3, 224, 224])
caption_ids: <class 'torch.Tensor'> torch.Size([2, 32])
video_id: <class 'list'> None
[DONE] loader smoke test finished.

## Step 6 | è®­ç»ƒæ¨¡åž‹

ðŸ“Œ ä»Šå¤©çš„é‡åˆ°çš„é—®é¢˜

æ•°æ®é›†è·¯å¾„ä¸ä¸€è‡´

annotations.json é‡Œ frames_dir å­—æ®µå¸¦æœ‰ -videoID_start_end æ ¼å¼ã€‚

ä½†å®žé™…æŠ½å¸§çš„ç›®å½•åå¤šæ˜¯ videoID æˆ– videoID_...ï¼Œå¯¼è‡´å¯¹ä¸ä¸Šã€‚

è¿‡æ»¤è„šæœ¬ kept: 0 çš„é—®é¢˜

åŽŸå› ä¸€ï¼šframes_dir è·¯å¾„é”™è¯¯ï¼Œæ²¡æœ‰å¯¹é½ã€‚

åŽŸå› äºŒï¼šè®¡æ•°å‡½æ•°ç”¨ glob(as_posix()) åœ¨ Windows ä¸‹å¤±æ•ˆï¼Œå¯¼è‡´å³ä¾¿ç›®å½•æœ‰å›¾ï¼Œä¹Ÿç»Ÿè®¡ä¸º 0ã€‚

è§£å†³ï¼šæ¢ç”¨ pathlib.iterdir() é‡æ–°å®žçŽ°è®¡æ•°ï¼Œç¡®è®¤èƒ½è¿”å›žæ­£ç¡®å¸§æ•°ã€‚

è®­ç»ƒæ—¶æŠ¥é”™ No frames found

å› ä¸º DataLoader é‡Œå†™æ­»åªåŒ¹é… frame_*.jpgã€‚

è§£å†³ï¼šæ”¹æˆæ›´é€šç”¨çš„ loader â†’ æ”¯æŒ *.jpg/*.png ç­‰åŽç¼€ï¼Œå¹¶å…è®¸é€’å½’æŸ¥æ‰¾ã€‚

captions ä½¿ç”¨é—®é¢˜

åŽŸä»£ç åªå–ç¬¬ä¸€æ¡ captionã€‚

å·²æ”¹è¿›ï¼šéšæœºé€‰ä¸€æ¡ captionï¼Œå¢žå¼ºè®­ç»ƒå¤šæ ·æ€§ã€‚

ä»Šå¤©çš„è¿›åº¦

çŽ¯å¢ƒå‡†å¤‡

base.yaml + .env å·²å®Œæˆï¼ŒåŸºæœ¬ä¾èµ–çŽ¯å¢ƒå‡†å¤‡å¥½ã€‚

æ•°æ®å¤„ç†

æŠ½å¸§éªŒè¯æˆåŠŸï¼Œå¯ä»¥ç”Ÿæˆ jpg å¸§ã€‚

è¡¥ä¸è„šæœ¬ + è¿‡æ»¤è„šæœ¬å®Œæˆï¼Œå·²æˆåŠŸä¿®å¤ä¸€éƒ¨åˆ†æ ·æœ¬è·¯å¾„ã€‚

train/annotations.filtered.json ç”Ÿæˆï¼Œå¹¶ç¡®è®¤æœ‰ kept: 97 å¯ç”¨æ ·æœ¬ã€‚

è®­ç»ƒæµç¨‹

ä¿®æ”¹ DataLoader â†’ å…¼å®¹å¤šæ ¼å¼å¸§æ–‡ä»¶ã€‚

æˆåŠŸè·‘é€š 20 ä¸ª batch çš„ smoke testï¼š
```python:
loss=6.9, steps=20
checkpoint -> checkpoints/msvd_debug/simple_vc_smoke.pt
```
- pipeline(æ•°æ®åŠ è½½->æ¨¡åž‹å‰å‘/åå‘ä¼ æ’­->å‚æ•°æ›´æ–°->checkpointä¿å­˜) é—­çŽ¯æ‰“é€šã€‚

#### å½“å‰è¿›åº¦å®šä½

å·²å®Œæˆï¼šå°è§„æ¨¡å®žéªŒé—­çŽ¯è·‘é€šï¼ˆsmoke testï¼‰ã€‚

å¾…æŽ¨è¿›ï¼š

æ‰©å¤§è¿‡æ»¤åŽçš„æ ·æœ¬é‡ï¼ˆä¿®è¡¥æ›´å¤š frames_dirï¼Œç›®æ ‡å‡ ç™¾~å‡ åƒæ¡ï¼‰ã€‚

æé«˜ --min_frames å›žåˆ° 8ï¼Œä¿è¯å¸§è´¨é‡ã€‚

æ‰©å¤§è®­ç»ƒè§„æ¨¡ï¼ˆæ›´å¤š batch/epochï¼‰ï¼Œè§‚å¯Ÿ loss æ›²çº¿ã€‚

å‡†å¤‡ inference æµ‹è¯•ï¼ŒéªŒè¯ caption è¾“å‡ºæ•ˆæžœã€‚

## Step 7 

```c:
>>> print("train annotations:",ann.as_posix())
train annotations: data/processed/msvd/train/annotations.json
>>> print("total:",len(recs),"with_frames:",ok)
total: 203 with_frames: 0
>>>

```
çŸ­è·‘äº†2~3ä¸ªbatch,æ­£å¸¸è¾“å‡ºã€‚
```c:
PS D:\Mylearn\INTI courses\Graduation thesis\video-captioning-transformer> python -m src.cli.train --ann_path data/processed/msvd/train/annotations.json --batch_size 2 --num_frame 8 --image_size 224 --max_len 32 --seed 123
[DEBUG] ann_path used by train: data/processed/msvd/train/annotations.json
[WARN] Dropped 176 samples without frames. kept=27
---- Batch 0 ----
video: torch.Size([2, 8, 3, 224, 224])
caption_ids: torch.Size([2, 32])
video_id: ['5JSbxHECb-I_97_110', '8PQiaurIiDM_94_99']
---- Batch 1 ----
video: torch.Size([2, 8, 3, 224, 224])
caption_ids: torch.Size([2, 32])
video_id: ['45AGQSbodbU_5_15', '1dfR0A_BXjw_590_600']
---- Batch 2 ----
video: torch.Size([2, 8, 3, 224, 224])
caption_ids: torch.Size([2, 32])
video_id: ['-pUwIypksfE_13_23', '8HB7ywgJuTg_131_142']

```

æå‡è¦†ç›–çŽ‡(éœ€è¦æå‡åˆ°80%ä»¥ä¸Šï¼Œä¿è¯è®­ç»ƒé‡çš„ç¨³å®šæ€§)
ä»Ž 13.3% åˆ°æ ·æœ¬é‡çš„ 
[DONE] split=train ok=169 fail=7
[COVERAGE] train: with_frames=196/203 (96.6%)

å¯ä»¥è·‘å…¨é‡çš„æ•°æ®æŒ‡ä»¤
```c:
# åªè¡¥ç¼ºï¼Œ4çº¿ç¨‹ï¼Œ2FPS
python -m scripts.extract_frames_mp --splits train --fps 2 --workers 4 --only-missing

# å¦‚æžœæƒ³å…¨é‡é‡æŠ½
python -m scripts.extract_frames_mp --splits train --fps 2 --workers 4 --overwrite

```
### è®­ç»ƒéªŒè¯çŽ¯èŠ‚

```c:

PS D:\Mylearn\INTI courses\Graduation thesis\video-captioning-transformer> python -m src.cli.train --ann_path data/processed/msvd/train/annotations.json --batch_size 2 --num_frame 8 --image_size 224 --max_len 32 --seed 123 --epochs 1 --max_steps 50 --lr 5e-4
[DEBUG] ann_path used by train: data/processed/msvd/train/annotations.json
[WARN] Dropped 7 samples without frames. kept=196
step 0001 | loss 1.0457
step 0010 | loss 0.9351
step 0020 | loss 0.6001
step 0030 | loss 0.5465
step 0040 | loss 0.4904
step 0050 | loss 0.7053

```