#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
check gpu 
å¿«é€Ÿæ£€æŸ¥PyTorch çš„GPU å¯ç”¨æ€§ä¸è®¾å¤‡çŠ¶æ€
å¯åœ¨è®­ç»ƒå‰è¿è¡Œç¡®è®¤GPUæ˜¯å¦åœ¨æ­£å¸¸ä½¿ç”¨

"""
import torch
import subprocess

def main():
    print("="*60)
    print("ğŸ” [GPU Check] PyTorchç¯å¢ƒä¸CUDAçŠ¶æ€")
    print("="*60)
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"[INFO] CUDA å¯ç”¨ | æ£€æµ‹åˆ° {device_count} ä¸ª GPU")
        
        for i in range(device_count):
            print(f"\n--- GPU {i} ---")
            print("è®¾å¤‡åç§°:",torch.cuda.get_device_name(i))
            print("æ˜¾å­˜æ€»é‡:{:.2f} GB".format(torch.cuda.get_device_properties(i).total_memory / 1024**3))
            print("å½“å‰å·²åˆ†é…æ˜¾å­˜: {:.2f} GB".format(torch.cuda.memory_allocated(i) / 1024**3))
            print("å½“å‰å·²ç¼“å­˜æ˜¾å­˜: {:.2f} GB".format(torch.cuda.memory_reserved(i) / 1024**3))
            
        print("\n[INFO] å½“å‰æ´»è·ƒè®¾å¤‡:",torch.cuda.current_device())
        print("[INFO] CUDA ç‰ˆæœ¬:",torch.version.cuda)
        print("[INFO] PyTorch ç‰ˆæœ¬:",torch.__version__)
        
    else:
        print("[WARN] âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨çš„ CUDA GPU , å½“å‰ä½¿ç”¨ CPU")
        print("[INFO] PyTorch ç‰ˆæœ¬:",torch.__version__)
        
    print("\n" + "="*60)
    print(" ğŸ’» nvidia-smi è¾“å‡º (å¦‚æœå·²å®‰è£… NVIDIA é©±åŠ¨):")
    print("="*60)
    try:
        subprocess.run(["nvidia-smi"],check=True)
    except Exception as e:
        print(f"[WARN] æ— æ³•æ‰§è¡Œ nvidia-smi: {e}")
        

if __name__ == "__main__":
    main()